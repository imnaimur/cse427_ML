# -*- coding: utf-8 -*-
"""CSE427_lab_project(Flat_Price_Justification_predictions).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Us6-KpgLTqftk6T27Kz8SNYyoYEOoWdB

##All imports here
"""

import numpy as np
import pandas as pd
import seaborn as sns
import lightgbm as lgbm
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import StackingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder , MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error

"""##File handling"""

import pandas as pd
file_link = 'https://drive.google.com/file/d/1BiYCkiIUS8NOmBaPYXilJISnWW7OnTrf/view?usp=sharing'
file_id = file_link.split('/')[-2]
new = f'https://drive.google.com/uc?id={file_id}'
df = pd.read_csv('property_bd.csv')
df.head()

df.info()
df.shape

"""##Pre-Processing

###Dropped unnecessary features
"""

df = df.drop(['image_url','property_description','property_overview','property_url','id','division','zone','address'],axis=1)
df.info()

df.isnull().sum()
df.nunique()

"""###Fitered data of Dhaka"""

df = df[
        (df["num_bed_rooms"] > 0) &
        (df["num_bath_rooms"] > 0) &
        (df["price"].between(10000, 4e7)) &
        (df['city']=='Dhaka') &
        (df['purpose'].str.lower() =='rent')
    ]
features = df.drop(['price','city','purpose'], axis=1)
target = df['price']
target_transformed = np.log(target)

df.shape

df.nunique()

"""###Splitting Data"""

X_train, X_test, y_train, y_test = train_test_split(features, target_transformed, test_size=0.2, random_state=42)
#seperating categorical and numerical cols
cat_cols=features.select_dtypes(include=['object']).columns.tolist()
num_cols = features.select_dtypes(include=np.number).columns.tolist()
print(cat_cols)
print(num_cols)

"""###Scaling and Encoding"""

scaler = MinMaxScaler()
encoder = OneHotEncoder(sparse_output=False,handle_unknown='ignore')
preprocessor = ColumnTransformer(
    transformers=[
        ("num", scaler, num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
    ])

"""###Co-relation Matrix"""

corr_matrix = df[num_cols + ["price"]].corr()

plt.figure(figsize=(10,6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlations with Price")
plt.show()


"""##Models

###Linear Regression
"""

linearRegression = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("regressor", LinearRegression())
    ]
)

linearRegression.fit(X_train, y_train)
y_pred = linearRegression.predict(X_test)
y_pred = np.exp(y_pred)
y_test_orig = np.exp(y_test)


lr_mae = mean_absolute_error(y_test_orig, y_pred)
lr_mse = mean_squared_error(y_test_orig, y_pred)
lr_r2 = r2_score(y_test_orig, y_pred)
lr_mape = mean_absolute_percentage_error(y_test_orig,y_pred)

print(f"R²   : {lr_r2:.3f}")
print(f"MAE  : {lr_mae:.2f}")
print(f"MAPE   : {lr_mape:.3f}")

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred, alpha=0.3, s=20,color='blue')
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r--', lw=2, label="Perfect Prediction")

plt.xlabel("Actual Price (BDT)")
plt.ylabel("Predicted Price (BDT)")
plt.title("Actual vs Predicted Flat Prices")
plt.legend()
plt.show()

"""###Important features plot"""

# # After fitting model with RandomForest
# rf_model = Pipeline(steps=[
#     ("preprocessor", preprocessor),
#     ("regressor", RandomForestRegressor(n_estimators=200, random_state=42))
# ])
# rf_model.fit(X_train, y_train)

# # Get feature names after preprocessing
# feature_names = (num_cols +
#                  list(preprocessor.transformers_[1][1] # OneHotEncoder
#                       .get_feature_names_out(cat_cols)))


# importances = rf_model.named_steps["regressor"].feature_importances_
# indices = np.argsort(importances)[::-1]

# plt.figure(figsize=(12,6))
# plt.bar(range(15), importances[indices][:15], align="center")
# plt.xticks(range(15), [feature_names[i] for i in indices[:15]], rotation=75)
# plt.title("Top 15 Feature Importances (RandomForest)")
# plt.tight_layout()
# plt.show()

"""###Random Forest Regression"""

rf_regressor = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("regressor", RandomForestRegressor(random_state=42))
    ]
)

param_grid = {
    "regressor__n_estimators": [100, 300, 500, 700],
    "regressor__max_depth": [5, 10, 15,20,25],
    "regressor__min_samples_split": [2, 5, 10,15]
}

randomForest = GridSearchCV(
    rf_regressor,
    param_grid,
    cv=3,
    scoring="r2",
    n_jobs=4
)


randomForest.fit(X_train, y_train)
y_pred = randomForest.predict(X_test)
y_pred = np.exp(y_pred)
y_test_orig = np.exp(y_test)


rf_mae = mean_absolute_error(y_test_orig, y_pred)
rf_mse = mean_squared_error(y_test_orig, y_pred)
rf_r2 = r2_score(y_test_orig, y_pred)
rf_mape = mean_absolute_percentage_error(y_test_orig,y_pred)

print("Best params:", randomForest.best_params_)
print("Best CV R²:", randomForest.best_score_)

print(f"R²   : {rf_r2:.3f}")
print(f"MAE  : {rf_mae:.2f}")
print(f"MAPE   : {rf_mape:.3f}")

"""###XGBR"""

xgb =Pipeline(
    steps = [
        ("preprocessor",preprocessor),
        ("regressor",XGBRegressor(n_estimators=1000, learning_rate=0.07, n_jobs=5))
])


xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)
y_pred = np.exp(y_pred)
y_test_orig = np.exp(y_test)


xgb_mae = mean_absolute_error(y_test_orig, y_pred)
xgb_mse = mean_squared_error(y_test_orig, y_pred)
xgb_r2 = r2_score(y_test_orig, y_pred)
xgb_mape = mean_absolute_percentage_error(y_test_orig,y_pred)

print(f"R²   : {xgb_r2:.3f}")
print(f"MAE  : {xgb_mae:.2f}")
print(f"MAPE   : {xgb_mape:.3f}")

"""###XGBR Tuned"""

xgb_t =Pipeline(
    steps = [
        ("preprocessor",preprocessor),
        ("regressor",XGBRegressor())
])

param_grid = {
    "regressor__n_estimators": [100, 300, 500, 700],
    "regressor__max_depth": [5, 10, 15,20,25],
    "regressor__learning_rate": [0.02, 0.05, 0.7, 0.8, 0.1],
    "regressor__n_jobs": [5, 6, 7]
}

xgb_tuned = GridSearchCV(
    xgb_t,
    cv=3,
    scoring="r2",
    n_jobs=4
)


xgb_tuned.fit(X_train, y_train)
y_pred = xgb_tuned.predict(X_test)



#Matrics
rf_r2= r2_score(y_test, y_pred)
rf_mse = mean_squared_error(y_test, y_pred)
rf_mae = mean_absolute_error(y_test, y_pred)
rf_mape = mean_absolute_percentage_error(y_test,y_pred)

print("Best params:", xgb_tuned.best_params_)
print("Best CV R²:", xgb_tuned.best_score_)

print(f"R²   : {rf_r2:.3f}")
print(f"MAE  : {rf_mae:.2f}")
print(f"MAPE   : {rf_mape:.3f}")

from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from xgboost import XGBRegressor

# Create a pipeline that includes the preprocessor and the XGBRegressor
cv_pipeline_xgb = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor(n_estimators=1000, learning_rate=0.07, n_jobs=5))
])

# Perform cross-validation on the original data using the pipeline
scores = cross_val_score(cv_pipeline_xgb, features, target, cv=5, scoring="r2")
print("CV R²:", scores.mean())

"""###Light GBM"""

import lightgbm as lgbm

lgbm_regressor = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("regressor", lgbm.LGBMRegressor(random_state=42))
    ]
)


lgbm_regressor.fit(X_train, y_train)
y_pred = lgbm_regressor.predict(X_test)
y_pred = np.exp(y_pred)
y_test_orig = np.exp(y_test)


lgbm_mae = mean_absolute_error(y_test_orig, y_pred)
lgbm_mse = mean_squared_error(y_test_orig, y_pred)
lgbm_r2 = r2_score(y_test_orig, y_pred)
lgbm_mape = mean_absolute_percentage_error(y_test_orig,y_pred)
score = cross_val_score(xgb, features, target, cv=5, scoring="r2")

print(f"R²   : {lgbm_r2:.3f}")
print(f"MAE  : {lgbm_mae:.2f}")
print(f"MAPE   : {lgbm_mape:.3f}")

"""##Performance Comparison

###R2 Score
"""

model_names = ["Linear Regression", "Random Forest", "XGBoost", "LightGBM"]
r2_scores = [
    lr_r2,      # Linear Regression
    rf_r2,      # Random Forest
    xgb_r2,     # XGBoost
    lgbm_r2     # LightGBM
]


plt.figure(figsize=(8,5))
bars = plt.bar(model_names, r2_scores, color=["skyblue", "orange", "green", "purple"])
plt.ylabel("R² Score")
plt.title("Model R² Score Comparison")
plt.ylim(0, 1)
plt.grid(axis="y", linestyle="--", alpha=0.5)

# Annotate bars
for bar in bars:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f"{bar.get_height():.3f}", ha="center", va="bottom", fontsize=10)

plt.show()

"""###MAE Score"""

mae = [
    lr_mae,      # Linear Regression
    rf_mae,      # Random Forest
    xgb_mae,     # XGBoost
    lgbm_mae     # LightGBM
]
model_names = ["Linear Regression", "Random Forest", "XGBoost", "LightGBM"]


plt.figure(figsize=(8,5))
bars = plt.bar(model_names, mae, color=["skyblue", "orange", "green", "purple"])
plt.ylabel("MAE Score")
plt.title("Model MAE Score Comparison")
# Remove or adjust this line:
# plt.ylim(0, 1)
plt.grid(axis="y", linestyle="--", alpha=0.5)

# Annotate bars
for bar in bars:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f"{bar.get_height():.3f}", ha="center", va="bottom", fontsize=10)

plt.show()

mape = [
    lr_mape,      # Linear Regression
    rf_mape,      # Random Forest
    xgb_mape,     # XGBoost
    lgbm_mape     # LightGBM
]
model_names = ["Linear Regression", "Random Forest", "XGBoost", "LightGBM"]


plt.figure(figsize=(8,5))
bars = plt.bar(model_names, mape, color=["skyblue", "orange", "green", "purple"])
plt.ylabel("MAE Score")
plt.title("Model MAPE Score Comparison")
# Remove or adjust this line:
# plt.ylim(0, 1)
plt.grid(axis="y", linestyle="--", alpha=0.5)

# Annotate bars
for bar in bars:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f"{bar.get_height():.3f}", ha="center", va="bottom", fontsize=10)

plt.show()

import pandas as pd

performance_df = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest", "XGBoost", "LightGBM"],
    "R2 Score": [lr_r2, rf_r2, xgb_r2, lgbm_r2],
    "MAE": [lr_mae, rf_mae, xgb_mae, lgbm_mae],
    "MAPE": [lr_mape, rf_mape, xgb_mape, lgbm_mape]
})

display(performance_df)

stack = StackingRegressor(
    estimators=[
        ('lr', Pipeline([('preprocessor', preprocessor), ('regressor', LinearRegression())])),
        ('rf', Pipeline([
            ('preprocessor', preprocessor),
            ('regressor', RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42))
        ])),
        ('xgb', Pipeline([
            ('preprocessor', preprocessor),
            ('regressor', XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42))
        ]))
    ],
    final_estimator=lgbm.LGBMRegressor()
)

stack.fit(X_train, y_train)
y_pred = stack.predict(X_test)
y_pred_original = np.expm1(y_pred)
y_test_original = np.expm1(y_test)

stack_r2 = r2_score(y_test_original, y_pred_original)
stack_mae = mean_absolute_error(y_test_original, y_pred_original)
stack_mape = mean_absolute_percentage_error(y_test_original, y_pred_original)

print(f"Stacked R²: {stack_r2:.3f}")
print(f"Stacked MAE: {stack_mae:.2f}")
print(f"Stacked MAPE: {stack_mape:.3f}")

print("Y_pred = ",y_pred.max())
print("Y_test = ",y_test.max())